{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c150d6b-85ee-499a-b587-53dff0d1fb9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_adls = \"abfss://bronze-layer@medallionapistorage.dfs.core.windows.net/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28f105af-b625-4eee-b242-27f1242b9209",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "import pandas as pd\n",
    "\n",
    "# Criando uma SparkSession\n",
    "spark = SparkSession.builder.appName(\"Open Brewery\").getOrCreate()\n",
    "\n",
    "# Cria dataframe spark com os dados persistidos na camada bronze\n",
    "df = spark.read.json(f\"{bronze_adls}open_breweries_raw.json\")\n",
    "data = df.toPandas()\n",
    "\n",
    "#df = spark.read.option(\"multiline\", \"true\").json(f\"{bronze_adls}open_breweries_raw.json\")\n",
    "#print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07a50107-a951-433c-892c-ad234ae75f69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Avaliação inicial dos dados disponíveis na API\n",
    "rows = df.count()\n",
    "columns = len(df.columns)\n",
    "print(f'A API traz {rows} linhas e {columns} colunas por página retornada')\n",
    "\n",
    "print(df.columns)\n",
    "print(df.dtypes)\n",
    "print(df.head())\n",
    "data.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c174e574-c21c-4af7-99d6-b73209235903",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transformação dos dados (limpeza, substituição de nulos e padronização de strings)\n",
    "\n",
    "# Verificação dos valores das colunas de endereço\n",
    "df.createOrReplaceTempView(\"API\")\n",
    "spark.sql(\"SELECT DISTINCT address_2, address_3 FROM API\").show()\n",
    "\n",
    "# Comparação das colunas state e state_province\n",
    "df_with_comparison = df.filter(col(\"state\") != col(\"state_province\"))\n",
    "df_with_comparison.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "exploratory_data_analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}